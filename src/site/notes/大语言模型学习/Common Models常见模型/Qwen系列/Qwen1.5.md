---
{"dg-publish":true,"dg-permalink":"/å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /Common-Modelså¸¸è§æ¨¡å‹/Qwenç³»åˆ—/Qwen1.5","dg-home":false,"dg-description":"åœ¨æ­¤è¾“å…¥ç¬”è®°çš„æè¿°","dg-hide":false,"dg-hide-title":false,"dg-show-backlinks":true,"dg-show-local-graph":true,"dg-show-inline-title":true,"dg-pinned":false,"dg-passphrase":"åœ¨æ­¤è¾“å…¥è®¿é—®å¯†ç ","dg-enable-mathjax":false,"dg-enable-mermaid":false,"dg-enable-uml":false,"dg-note-icon":0,"dg-enable-dataview":false,"tags":["NLP"],"permalink":"/å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ /Common-Modelså¸¸è§æ¨¡å‹/Qwenç³»åˆ—/Qwen1.5/","dgShowBacklinks":true,"dgShowLocalGraph":true,"dgShowInlineTitle":true,"dgPassFrontmatter":true,"noteIcon":0,"created":"2025-04-25T11:13:40.534+08:00","updated":"2025-04-25T11:15:43.034+08:00"}
---



# Qwen1.5 æ¨¡å‹è§£æä¸æŠ€æœ¯åˆ›æ–°

## å…ƒæ•°æ®
- åˆ†ç±»ï¼šäººå·¥æ™ºèƒ½
- æ ‡ç­¾ï¼šQwen1.5, æ¨¡å‹ç»“æ„, MoE, æ·±åº¦å­¦ä¹ , è‡ªç„¶è¯­è¨€å¤„ç†
- æ—¥æœŸï¼š2025å¹´4æœˆ12æ—¥


## å†…å®¹æ€»ç»“
Qwen1.5 æ˜¯ä¸€ç§å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œå…¶ç»“æ„å’Œè®­ç»ƒæ–¹æ³•åœ¨å¤šä¸ªæ–¹é¢è¿›è¡Œäº†åˆ›æ–°å’Œä¼˜åŒ–ã€‚æœ¬æ–‡å°†è¯¦ç»†è§£æè¯¥æ¨¡å‹çš„å…³é”®æŠ€æœ¯ç‚¹åŠå…¶å®ç°ç»†èŠ‚ã€‚


## æ¨¡å‹ç»“æ„
Qwen1.5 ä½¿ç”¨äº†å¤šç§ä¼˜åŒ–æŠ€æœ¯æ¥æå‡æ€§èƒ½ï¼š

- **Tokenizer å’Œæ¨¡å‹ç»„ä»¶**ï¼šä½¿ç”¨äº† BPEã€PreRMSNormã€SwiGLUã€RoPE ä½œä¸ºåŸºç¡€ç»„ä»¶ï¼Œå½¢æˆäº†æ‰€è°“çš„â€œé»„é‡‘å››ä»¶å¥—â€ã€‚
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šç»§ç»­ä½¿ç”¨ Flash Attentionï¼Œå¹¶é€šè¿‡ `torch.nn.functional.scaled_dot_product_attention` å®ç° SDPA Attentionã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒQwen1.5-32B ç‰ˆæœ¬å¼•å…¥äº† GQAï¼ˆGated Query Attentionï¼‰ã€‚
- **å…¶ä»–ç»“æ„æ”¹åŠ¨**ï¼šæ¨¡å‹çš„ Embedding å’Œè¾“å‡ºå±‚æ²¡æœ‰è¿›è¡Œå‚æ•°å…±äº«ï¼ˆ`tie_word_embedding=false`ï¼‰ï¼Œè€Œæ˜¯ä»…åœ¨ QKV å‚æ•°ä¸­æ·»åŠ äº†åç½®ã€‚

ğŸ’¡ **å¯å‘ç‚¹**ï¼šQwen1.5 çš„è®¾è®¡ä¸­ï¼Œç»†ç²’åº¦ä¸“å®¶æœºåˆ¶å’Œä¸å…±äº«å‚æ•°çš„ç­–ç•¥å¯èƒ½ä¸ºæ¨¡å‹æ€§èƒ½å¸¦æ¥äº†æ˜¾è‘—æå‡ã€‚


## ç»†ç²’åº¦ä¸“å®¶æœºåˆ¶
Qwen1.5-MoE-A2.7B å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦ä¸“å®¶æœºåˆ¶ï¼š

- **ä¸“å®¶åˆ’åˆ†**ï¼šå°† FFNï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œï¼‰åˆ‡åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ä¸“å®¶ã€‚
- **åˆå§‹åŒ–ç­–ç•¥**ï¼šåˆ©ç”¨ Qwen-1.8B è¿›è¡Œåˆå§‹åŒ–æ”¹é€ ï¼Œå¹¶å¼•å…¥éšæœºæ€§ä»¥åŠ é€Ÿæ”¶æ•›ã€‚
- **è·¯ç”±æœºåˆ¶**ï¼šè®¾ç½®å››ä¸ªå…±äº«ä¸“å®¶å’Œ 60 ä¸ªè·¯ç”±ä¸“å®¶ï¼Œæ¯æ¬¡æ¿€æ´»å››ä¸ªä¸“å®¶ã€‚


## æ¨¡å‹è®­ç»ƒ
è®­ç»ƒæ•°æ®é‡æœªå…¬å¸ƒï¼Œä½†åœ¨åå¥½å¯¹é½éƒ¨åˆ†ä¸­ï¼Œä½¿ç”¨äº† PPO å’Œ DPO æ–¹æ³•ã€‚Qwen1.5 æ”¯æŒ 32K ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›äº† AWQ å’Œ GPTQ çš„é‡åŒ–æ¨¡å‹ï¼ŒåŒ…æ‹¬ int4 å’Œ int8 æ ¼å¼ã€‚


## ä»£ç æ¼”ç¤º
**PreTrainedModel.from_pretrainedè°ƒç”¨tie_weightsæ–¹æ³•**

```python
# fromhttps://github.com/huggingface/transformers/blob/ee339bad01bf09266eba665c5f063f0ab7474dad/src/transformers/modeling_utils.py#L1264
def tie_weights(self):
    """
    Tie the weights between the input embeddings and the output embeddings.
    
    If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
    weights instead.
    """
    if getattr(self.config, "tie_word_embeddings", True):
        output_embeddings = self.get_output_embeddings()
        if output_embeddings is not None:
            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())
    
    if getattr(self.config, "is_encoder_decoder", False) and getattr(self.config, "tie_encoder_decoder", False):
        if hasattr(self, self.base_model_prefix):
            self = getattr(self, self.base_model_prefix)
        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)
    
    for module in self.modules():
        if hasattr(module, "_tie_weights"):
            module._tie_weights()

```

### å¸¸è§é”™è¯¯
> åœ¨å®ç°æ¨¡å‹æ—¶ï¼Œæ³¨æ„ä¸è¦æ··æ·†å‚æ•°å…±äº«è®¾ç½®ï¼Œç¡®ä¿ `tie_word_embedding` å‚æ•°æ­£ç¡®é…ç½®ã€‚


### æ“ä½œæ­¥éª¤
1. âœ… åˆå§‹åŒ– Qwen1.5 ä½¿ç”¨ Qwen-1.8Bã€‚
2. âš  åˆ†å‰² FFN ä¸ºå¤šä¸ªç‹¬ç«‹ä¸“å®¶ã€‚
3. â— ä½¿ç”¨éšæœºæ€§åŠ é€Ÿæ”¶æ•›ã€‚
4. âœ… é…ç½®è·¯ç”±æœºåˆ¶ä»¥æ¿€æ´»é€‚å½“æ•°é‡çš„ä¸“å®¶ã€‚


### æ•°æ®è¡¨æ ¼
| å‚æ•°              | æè¿°                   |
|-----------------|----------------------|
| Tokenizer       | BPE, PreRMSNorm, SwiGLU, RoPE |
| æ³¨æ„åŠ›æœºåˆ¶         | Flash Attention, SDPA Attention |
| æ¨¡å‹ç‰ˆæœ¬          | Qwen1.5-32B ä½¿ç”¨ GQA  |


### è¡ŒåŠ¨æ¸…å•
- æ¢ç´¢ Qwen1.5 åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚
- æ·±å…¥ç ”ç©¶ç»†ç²’åº¦ä¸“å®¶æœºåˆ¶å¯¹æ¨¡å‹æ•ˆç‡çš„å½±å“ã€‚
- å°è¯•åœ¨å…¶ä»–æ¨¡å‹ä¸­åº”ç”¨ä¸å…±äº«å‚æ•°çš„ç­–ç•¥ã€‚

> åŸå§‹å‡ºå¤„ï¼š[Qwen1.5 å®˜æ–¹åšå®¢](https://qwenlm.github.io/zh/blog/qwen1.5/)

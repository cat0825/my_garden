## **关于大语言模型学习导航**
- [大语言模型学习](#大语言模型学习)
  - [Attention注意力机制](#attention注意力机制)
    - [Attention机制详解与应用](#attention机制详解与应用)
    - [DCA：长文本处理的新突破（Dual Chunk Attention）](#dca长文本处理的新突破dual-chunk-attention)
    - [KV Cache技术详解：优化Transformer自回归生成效率](#kv-cache技术详解优化transformer自回归生成效率)
    - [Transformer中的Attention详解与应用指南](#transformer中的attention详解与应用指南)
    - [【长上下文模型优化】基于Shifted Sparse Attention的创新方法](#长上下文模型优化基于shifted-sparse-attention的创新方法)
    - [优化Attention计算复杂度的技术探讨](#优化attention计算复杂度的技术探讨)
    - [深度学习中的注意力机制优化：从MHA到MLA](#深度学习中的注意力机制优化从mha到mla)
  - [FFN、Add & LN 的作用与应用](#ffnadd-ln-的作用与应用)
    - [Transformer核心模块解析：FFN、Add & LN 的作用与应用](#transformer核心模块解析ffnadd-ln-的作用与应用)
    - [深度学习中的Layer Norm设计：Post-Norm、Pre-Norm与Sandwich-Norm比较](#深度学习中的layer-norm设计post-normpre-norm与sandwich-norm比较)
    - [激活函数与FFN结构优化：SwiGLU、GeGLU及其应用解析](#激活函数与ffn结构优化swiglugeglu及其应用解析)
    - [激活函数详解与比较：从Sigmoid到Swish](#激活函数详解与比较从sigmoid到swish)
  - [Positional Encoding位置编码](#positional-encoding位置编码)
    - [NTK插值方法解析与优化：从NTK-aware到NTK-by-parts](#ntk插值方法解析与优化从ntk-aware到ntk-by-parts)
    - [YaRN方法解析：扩展RoPE嵌入与注意力优化的实践](#yarn方法解析扩展rope嵌入与注意力优化的实践)
    - [介绍|位置编码介绍](#介绍位置编码介绍)
    - [位置内插法扩展语言模型上下文长度](#位置内插法扩展语言模型上下文长度)
    - [数字输入优化与外推方法解析](#数字输入优化与外推方法解析)
    - [旋转位置编码与ALiBi：深度学习中的位置嵌入优化](#旋转位置编码与alibi深度学习中的位置嵌入优化)
    - [相对位置编码](#相对位置编码)
      - [DeBERTa的相对位置编码与绝对位置编码解析](#deberta的相对位置编码与绝对位置编码解析)
      - [T5模型与相对位置编码优化解析](#t5模型与相对位置编码优化解析)
      - [相对位置编码与XLNet位置编码详解 深入理解Transformer机制](#相对位置编码与xlnet位置编码详解深入理解transformer机制)
    - [绝对位置编码](#绝对位置编码)
      - [BERT与RNN位置编码的对比与应用](#bert与rnn位置编码的对比与应用)
      - [Transformer绝对位置编码详解与改进分析](#transformer绝对位置编码详解与改进分析)
  - [Pre-training 预训练](#pre-training-预训练)
    - [推理耗时](#推理耗时)
    - [数据多样性与模型优化探索](#数据多样性与模型优化探索)
    - [数据清洗](#数据清洗)
    - [数据爬取](#数据爬取)
    - [数据配比与训练顺序优化指南](#数据配比与训练顺序优化指南)
    - [模型打分与数据去重](#模型打分与数据去重)
    - [深度学习中的显存优化与梯度处理方法](#深度学习中的显存优化与梯度处理方法)
    - [混合精度训练](#混合精度训练)
    - [继续预训练](#继续预训练)
    - [训练容灾及训练监控](#训练容灾及训练监控)
    - [预训练定义以及数据来源](#预训练定义以及数据来源)
    - [预训练评估](#预训练评估)
    - [预训练评估2](#预训练评估2)
    - [预训练过程](#预训练过程)
      - [训练Tokenizer](#训练tokenizer)
      - [预训练的Scaling Law](#预训练的scaling-law)
      - [预训练策略](#预训练策略)
      - [高效深度学习模型训练框架选择与优化指南](#高效深度学习模型训练框架选择与优化指南)
  - [RL强化学习基础](#rl强化学习基础)
    - [SARSA-λ与Q-learning对比](#sarsa-λ与q-learning对比)
    - [SARSA算法](#sarsa算法)
    - [价值迭代算法](#价值迭代算法)
    - [强化学习分类](#强化学习分类)
    - [强化学习的独特性](#强化学习的独特性)
    - [强化学习问题,流程](#强化学习问题流程)
    - [时序差分算法](#时序差分算法)
    - [深度Q网络](#深度q网络)
    - [策略迭代算法](#策略迭代算法)
    - [蒙特卡洛方法](#蒙特卡洛方法)
    - [贝尔曼方程](#贝尔曼方程)
    - [马尔可夫决策过程](#马尔可夫决策过程)
  - [Structure & Decoding Policy 结构和解码策略](#structure-decoding-policy-结构和解码策略)
    - [大模型结构与混合专家（LLM & MoE）解析](#大模型结构与混合专家llm-moe-解析)
    - [深度解析语言模型采样方法：Top-K、Top-P、Temperature及综合策略](#深度解析语言模型采样方法top-k-top-p-temperature及综合策略)
    - [解码采样策略：Greedy Search与Beam Search的实现与优化](#解码采样策略greedy-search与beam-search的实现与优化)
  - [分词](#分词)
    - [BBPE：字节级别的BPE分词技术解析与应用](#bbpe字节级别的bpe分词技术解析与应用)
    - [WordPiece分词算法解析与实践](#wordpiece分词算法解析与实践)
    - [使用Byte Pair Encoding (BPE)优化子词分词的技巧与实践](#使用byte-pair-encoding-bpe优化子词分词的技巧与实践)
    - [使用Unigram语言模型（ULM）优化分词算法：核心思路与实践](#使用unigram语言模型ulm优化分词算法核心思路与实践)
    - [分词算法的比较](#分词算法的比较)
    - [常用分词库](#常用分词库)
  - [后训练](#后训练)
    - [SFT监督微调](#sft监督微调)
      - [SFT数据及处理](#sft数据及处理)
        - [开源数据集](#开源数据集)
        - [数据多样性探索](#数据多样性探索)
        - [数据生产合成与质量过滤](#数据生产合成与质量过滤)
        - [数据飞轮在SFT中的应用与优化](#数据飞轮在sft中的应用与优化)
      - [STF训练](#stf训练)
        - [多轮对话专项提升](#多轮对话专项提升)
        - [多轮对话专项提升2](#多轮对话专项提升2)
        - [训练启动脚本](#训练启动脚本)
        - [训练技巧和训练策略](#训练技巧和训练策略)
        - [训练框架及参数设置](#训练框架及参数设置)
      - [监督微调与预训练的区别](#监督微调与预训练的区别)
  - [词嵌入](#词嵌入)
    - [FastText](#fasttext)
    - [oneHot](#onehot)
    - [Word2Vec](#word2vec)
    - [介绍|词嵌入介绍](#介绍词嵌入介绍)
